{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ddebb5-f4a4-425d-b638-e2eb88890311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 1\n",
    "# Ans --In mathematics and data analysis, a projection is a transformation that maps one vector or set of vectors onto another vector or subspace. In the context of Principal Component Analysis (PCA), projection plays a fundamental role in reducing the dimensionality of a dataset.\n",
    "\n",
    "Here's how projection is used in PCA:\n",
    "\n",
    "1. **Centering the Data**:\n",
    "   - The first step in PCA is to center the data by subtracting the mean of each feature from the data points. This centers the data around the origin.\n",
    "\n",
    "2. **Computing Covariance Matrix**:\n",
    "   - Next, the covariance matrix of the centered data is computed. The covariance matrix quantifies the relationships between different features in the data.\n",
    "\n",
    "3. **Eigenvalue Decomposition**:\n",
    "   - The covariance matrix is then decomposed into its eigenvectors and eigenvalues. The eigenvectors represent the directions (or axes) along which the data varies the most, and the eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "4. **Selecting Principal Components**:\n",
    "   - The eigenvectors are ranked based on their corresponding eigenvalues in decreasing order. The eigenvectors with the highest eigenvalues are the principal components, which capture the most important directions of variability in the data.\n",
    "\n",
    "5. **Projection**:\n",
    "   - To reduce the dimensionality of the data, you select a subset of the top principal components (usually in order of decreasing eigenvalues). Then, you project the original data onto this subspace defined by the selected principal components.\n",
    "\n",
    "   - Mathematically, this is achieved by taking the dot product of the centered data with the selected eigenvectors. Each data point is projected onto the subspace defined by the selected principal components.\n",
    "\n",
    "   - The result is a lower-dimensional representation of the data, where each dimension corresponds to a principal component.\n",
    "\n",
    "6. **Reconstruction** (Optional):\n",
    "   - If needed, you can reconstruct the data back into the original high-dimensional space by taking the inverse projection. This allows you to understand how well the reduced-dimensional representation captures the original data.\n",
    "\n",
    "Projection in PCA allows for the transformation of data from a high-dimensional space to a lower-dimensional space while retaining as much of the original variance as possible. This enables more efficient storage, visualization, and analysis of data, and often helps improve the performance of machine learning models by reducing noise and focusing on the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c6bf6-c35e-469a-9b94-6e09350dc4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 2\n",
    "# Ans --The optimization problem in Principal Component Analysis (PCA) revolves around finding the eigenvectors (principal components) of the covariance matrix of the centered data. Let's break down how this optimization problem works and what it aims to achieve:\n",
    "\n",
    "1. **Covariance Matrix**:\n",
    "   - PCA starts by computing the covariance matrix of the centered data. The covariance matrix summarizes the relationships between different features, indicating how they vary together.\n",
    "\n",
    "2. **Eigenvalue Decomposition**:\n",
    "   - The covariance matrix is then subjected to eigenvalue decomposition, which yields a set of eigenvectors and eigenvalues. The eigenvectors represent the directions (axes) in the original feature space along which the data varies the most. The eigenvalues correspond to the amount of variance explained by each eigenvector.\n",
    "\n",
    "3. **Selecting Principal Components**:\n",
    "   - The eigenvectors are ranked based on their corresponding eigenvalues in decreasing order. The eigenvectors with the highest eigenvalues are the principal components, as they capture the most significant sources of variation in the data.\n",
    "\n",
    "4. **Optimization Objective**:\n",
    "   - The optimization objective in PCA is to find the set of eigenvectors (principal components) that maximizes the total variance captured by these components.\n",
    "\n",
    "   - In other words, PCA aims to find the linear subspace in which the data can be best represented, such that projecting the data onto this subspace retains as much of the original variance as possible.\n",
    "\n",
    "   - Mathematically, this can be formulated as:\n",
    "\n",
    "     Maximize \\(\\frac{{\\sum_{i=1}^{k} \\lambda_i}}{{\\sum_{i=1}^{n} \\lambda_i}}\\)\n",
    "\n",
    "     where:\n",
    "     - \\(k\\) is the number of selected principal components.\n",
    "     - \\(\\lambda_i\\) is the \\(i\\)-th eigenvalue.\n",
    "\n",
    "   - This objective is equivalent to maximizing the proportion of the total variance explained by the selected principal components.\n",
    "\n",
    "5. **Orthogonality Constraint**:\n",
    "   - The selected principal components must be orthogonal to each other. This constraint ensures that they capture different sources of variation in the data.\n",
    "\n",
    "6. **Normalization Constraint**:\n",
    "   - Typically, the eigenvectors are normalized to unit length. This ensures that the scale of the eigenvectors doesn't impact their contribution to the total variance.\n",
    "\n",
    "By solving this optimization problem, PCA identifies the optimal set of eigenvectors (principal components) that best represent the data in a lower-dimensional space while maximizing the retained variance. This enables more efficient storage, visualization, and analysis of data, and often helps improve the performance of machine learning models by reducing noise and focusing on the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55724d29-ed43-4851-81ed-8f975a0d7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 3\n",
    "# Ans --The relationship between covariance matrices and Principal Component Analysis (PCA) is fundamental to understanding how PCA works. Here's how they are connected:\n",
    "\n",
    "1. **Covariance Matrix**:\n",
    "\n",
    "   - The covariance matrix is a symmetric matrix that quantifies the relationships between different features in a dataset. Specifically, it provides information about how pairs of features co-vary or vary together.\n",
    "\n",
    "   - For a dataset with \\(n\\) features, the covariance matrix \\(\\Sigma\\) is an \\(n \\times n\\) matrix. The element in the \\(i\\)-th row and \\(j\\)-th column represents the covariance between the \\(i\\)-th and \\(j\\)-th features.\n",
    "\n",
    "   - Mathematically, the covariance matrix is computed as:\n",
    "\n",
    "     \\[\n",
    "     \\Sigma = \\frac{1}{m} \\sum_{i=1}^{m} (x^{(i)} - \\mu)(x^{(i)} - \\mu)^T\n",
    "     \\]\n",
    "\n",
    "     where \\(m\\) is the number of data points, \\(x^{(i)}\\) is a data point, and \\(\\mu\\) is the mean vector of the feature values.\n",
    "\n",
    "2. **PCA and Covariance Matrix**:\n",
    "\n",
    "   - PCA aims to find the directions (principal components) along which the data varies the most. These directions are identified by solving the eigenvector problem for the covariance matrix.\n",
    "\n",
    "   - Specifically, the eigenvectors of the covariance matrix represent these directions, and the corresponding eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "   - The \\(i\\)-th eigenvector corresponds to the \\(i\\)-th principal component, and its associated eigenvalue represents the amount of variance explained by that component.\n",
    "\n",
    "   - The principal components are the directions of maximal variance in the original feature space, and they are the basis vectors of the linear subspace onto which the data will be projected.\n",
    "\n",
    "3. **Dimensionality Reduction with PCA**:\n",
    "\n",
    "   - PCA performs dimensionality reduction by selecting a subset of the top principal components (eigenvectors with the highest eigenvalues). These principal components define a lower-dimensional subspace.\n",
    "\n",
    "   - Projecting the data onto this subspace retains as much of the original variance as possible, effectively capturing the most important information in the data.\n",
    "\n",
    "   - The projection operation is performed by taking the dot product of the centered data with the selected eigenvectors.\n",
    "\n",
    "In summary, the covariance matrix is central to PCA because it provides the information needed to identify the directions of maximal variance in the data. Solving the eigenvector problem for the covariance matrix allows PCA to find the optimal set of principal components that best represent the data in a lower-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9391b-3712-44bb-8b0f-ee9f156e33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 4\n",
    "# Ans-The choice of the number of principal components in PCA has a significant impact on the performance and effectiveness of the dimensionality reduction process. Here's how it influences PCA:\n",
    "\n",
    "1. **Amount of Variance Retained**:\n",
    "\n",
    "   - The number of principal components chosen directly affects the amount of variance retained in the reduced-dimensional representation. Selecting more principal components typically preserves more of the original variance.\n",
    "\n",
    "   - With more principal components, the representation is closer to the original data, but it also means retaining more dimensions and potentially not achieving substantial dimensionality reduction.\n",
    "\n",
    "2. **Information Loss**:\n",
    "\n",
    "   - Choosing fewer principal components can lead to more information loss, as the representation may not capture all the details and nuances present in the original data.\n",
    "\n",
    "   - On the other hand, selecting too many principal components may lead to minimal information loss but may not effectively reduce dimensionality.\n",
    "\n",
    "3. **Overfitting and Generalization**:\n",
    "\n",
    "   - If too many principal components are retained, the reduced-dimensional representation may still contain noise or irrelevant features. This can lead to overfitting in downstream machine learning models, as they may learn from noise in the data.\n",
    "\n",
    "   - Conversely, if too few principal components are chosen, important information may be lost, leading to underfitting and poor generalization performance.\n",
    "\n",
    "4. **Computational Efficiency**:\n",
    "\n",
    "   - The number of principal components selected impacts the computational resources required for further analysis or modeling. Choosing fewer components reduces the computational burden, making it faster to process and analyze the data.\n",
    "\n",
    "5. **Visualization and Interpretability**:\n",
    "\n",
    "   - When using PCA for visualization or interpretation purposes, selecting a small number of principal components allows for more straightforward visualizations and easier interpretation of the reduced-dimensional representation.\n",
    "\n",
    "6. **Balancing Trade-offs**:\n",
    "\n",
    "   - Selecting the optimal number of principal components involves finding a balance between retaining enough information for the task at hand while achieving effective dimensionality reduction.\n",
    "\n",
    "   - Techniques like the scree plot, explained variance, cross-validation, and domain knowledge can help in determining the right number of components.\n",
    "\n",
    "In summary, the choice of the number of principal components in PCA is a crucial decision that should be based on the specific characteristics of the dataset, the goals of the analysis, and the computational resources available. It's often advisable to try different numbers and evaluate their impact on the final model's performance before making a final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c5cfc1-65fc-4fae-923b-0488e1ba1de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 5\n",
    "# Ans --PCA can be used for feature selection by identifying a subset of the original features (or variables) that capture the most important information in the data. Here's how PCA is applied for feature selection and the benefits of using it for this purpose:\n",
    "\n",
    "1. **Step-by-Step Process**:\n",
    "\n",
    "   - **Step 1: Data Preprocessing**:\n",
    "     - Standardize or normalize the data to ensure that features are on similar scales. This is important because PCA is sensitive to the relative scales of features.\n",
    "\n",
    "   - **Step 2: Perform PCA**:\n",
    "     - Apply PCA to the preprocessed data to find the principal components. This involves computing the covariance matrix, performing eigenvalue decomposition, and selecting the top \\(k\\) eigenvectors.\n",
    "\n",
    "   - **Step 3: Select Principal Components**:\n",
    "     - Choose the top \\(k\\) principal components that capture a significant amount of variance in the data. The exact number of components to select depends on the desired level of dimensionality reduction.\n",
    "\n",
    "   - **Step 4: Project Data**:\n",
    "     - Project the original data onto the selected principal components. This yields a reduced-dimensional representation of the data.\n",
    "\n",
    "   - **Step 5: Interpret Results**:\n",
    "     - Examine the loadings (weights) of the original features on the selected principal components. Features with high absolute loadings are the most influential in defining the principal components.\n",
    "\n",
    "   - **Step 6: Feature Selection**:\n",
    "     - Based on the loadings, select the original features that contribute significantly to the top principal components. These selected features can form the reduced feature set.\n",
    "\n",
    "2. **Benefits of Using PCA for Feature Selection**:\n",
    "\n",
    "   - **Dimensionality Reduction**: PCA reduces the dimensionality of the feature space while retaining as much relevant information as possible. This leads to more efficient storage, processing, and analysis of data.\n",
    "\n",
    "   - **Collinearity Handling**: PCA handles collinearity (high correlation between features) effectively. By expressing features in terms of uncorrelated principal components, multicollinearity issues are mitigated.\n",
    "\n",
    "   - **Noise Reduction**: PCA can help in reducing noise and focusing on the most important sources of variation in the data. This is particularly beneficial for improving the performance of machine learning models.\n",
    "\n",
    "   - **Interpretability**: The reduced set of features obtained from PCA may be more interpretable than the original set, as they represent combinations of the original features that capture the most significant information.\n",
    "\n",
    "   - **Visualization**: If the data needs to be visualized, the reduced-dimensional representation obtained from PCA allows for easy visualization in 2D or 3D plots, which can be challenging with a large number of features.\n",
    "\n",
    "   - **Preprocessing for Downstream Models**: The reduced feature set obtained from PCA can serve as a more focused input for subsequent modeling tasks, potentially improving the performance of these models.\n",
    "\n",
    "   - **Simplicity and Efficiency**: The process of feature selection using PCA is relatively straightforward, making it an efficient technique for dimensionality reduction and feature extraction.\n",
    "\n",
    "Overall, PCA is a powerful tool for feature selection when the goal is to retain the most relevant information while reducing the dimensionality of the feature space. It can lead to more effective and efficient data analysis and modeling processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c0402-617b-49b0-877b-4761e81a8d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 6\n",
    "# Ans --Principal Component Analysis (PCA) has a wide range of applications in data science and machine learning across various domains. Here are some common applications:\n",
    "\n",
    "1. **Dimensionality Reduction**:\n",
    "   - PCA is extensively used to reduce the number of features in high-dimensional datasets while retaining as much relevant information as possible. This is beneficial for tasks like visualization, preprocessing, and improving the performance of machine learning models.\n",
    "\n",
    "2. **Image Processing**:\n",
    "   - In image analysis, PCA can be applied to reduce the dimensionality of image data. It's used in tasks like face recognition, image compression, and denoising.\n",
    "\n",
    "3. **Face Recognition**:\n",
    "   - PCA has been used for face recognition by representing faces as points in a high-dimensional space and then identifying the principal components that capture the most important facial features.\n",
    "\n",
    "4. **Speech Recognition**:\n",
    "   - PCA can be applied to acoustic features in speech processing to reduce noise and improve the performance of speech recognition systems.\n",
    "\n",
    "5. **Bioinformatics**:\n",
    "   - PCA is used for analyzing high-dimensional biological data, such as gene expression data, to identify patterns and reduce noise.\n",
    "\n",
    "6. **Finance and Economics**:\n",
    "   - PCA is applied in portfolio optimization to identify a set of uncorrelated factors (principal components) that explain most of the variation in asset returns. It's also used in risk assessment and modeling financial time series.\n",
    "\n",
    "7. **Chemometrics**:\n",
    "   - In chemistry, PCA is used for analyzing complex chemical datasets, such as spectroscopy data, to identify underlying chemical patterns.\n",
    "\n",
    "8. **Neuroscience**:\n",
    "   - PCA is used to analyze neuroimaging data, such as functional MRI (fMRI) and EEG signals, to identify patterns of brain activity and reduce noise.\n",
    "\n",
    "9. **Anomaly Detection**:\n",
    "   - PCA can be used to identify outliers or anomalies in datasets by projecting data onto the subspace defined by the principal components. Outliers are likely to have large reconstruction errors.\n",
    "\n",
    "10. **Recommendation Systems**:\n",
    "    - PCA can be used in collaborative filtering methods for recommendation systems to reduce the dimensionality of user-item interaction matrices and improve the efficiency of recommendations.\n",
    "\n",
    "11. **Natural Language Processing (NLP)**:\n",
    "    - PCA can be applied to reduce the dimensionality of term-document matrices in text analysis, helping in tasks like topic modeling and document clustering.\n",
    "\n",
    "12. **Spectral Analysis**:\n",
    "    - PCA can be used in signal processing for analyzing spectral data, such as in remote sensing or audio processing.\n",
    "\n",
    "13. **Customer Segmentation**:\n",
    "    - PCA can be applied in marketing to segment customers based on their purchasing behavior, helping businesses tailor marketing strategies.\n",
    "\n",
    "14. **Climate Science**:\n",
    "    - PCA is used to analyze climate data to identify patterns, trends, and variability in large-scale climate processes.\n",
    "\n",
    "These applications demonstrate the versatility of PCA in a wide range of fields, making it a valuable tool for data analysis and preprocessing in various domains of science and industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670db2ed-3bc7-4920-968e-5fe7039ce65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 7 \n",
    "# Ans --In Principal Component Analysis (PCA), \"spread\" and \"variance\" are closely related concepts, and they both refer to the extent of variability or dispersion of data points in different directions. Here's how they are related:\n",
    "\n",
    "1. **Variance**:\n",
    "\n",
    "   - **Definition**: Variance is a statistical measure that quantifies the spread or dispersion of a set of data points around their mean. It is calculated as the average of the squared differences between each data point and the mean.\n",
    "\n",
    "   - **PCA and Variance**: In PCA, the eigenvalues of the covariance matrix represent the variances along the principal components. Each eigenvalue corresponds to the amount of variance explained by its associated principal component.\n",
    "\n",
    "   - **Eigenvalues and Spread**: Larger eigenvalues indicate that the data points have a higher spread or variability along the corresponding principal component. This means that the principal component captures more of the overall variability in the data.\n",
    "\n",
    "2. **Spread**:\n",
    "\n",
    "   - **Definition**: Spread is a more general term that refers to the extent or range over which data points are distributed. It doesn't necessarily have to be quantified using a specific mathematical measure like variance.\n",
    "\n",
    "   - **PCA and Spread**: In PCA, the spread of data points along a particular direction is represented by the standard deviation of the data when projected onto that direction. The standard deviation is the square root of the corresponding eigenvalue.\n",
    "\n",
    "   - **Eigenvalues and Spread**: Larger eigenvalues indicate a greater spread of data along the corresponding principal component. This means that the principal component captures more of the variability in that direction.\n",
    "\n",
    "In summary, in the context of PCA:\n",
    "\n",
    "- **Variance** refers specifically to the statistical measure of spread around the mean, and it is represented by eigenvalues in PCA.\n",
    "  \n",
    "- **Spread** is a more general term that can refer to the extent of variability or dispersion in any direction, and it is related to the standard deviation along a particular direction.\n",
    "\n",
    "Both concepts are important in PCA because they provide information about the directions in which the data varies the most (captured by principal components) and the amount of variability or spread along those directions. The eigenvalues, which represent variances, play a crucial role in determining the significance of each principal component in capturing the underlying structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166647ae-acf9-401d-b72a-a0db4b89dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 8 \n",
    " # Ans --Principal Component Analysis (PCA) uses the spread and variance of the data to identify the principal components through the eigenvalue decomposition of the covariance matrix. Here's how it works:\n",
    "\n",
    "1. **Covariance Matrix**:\n",
    "\n",
    "   - PCA starts by computing the covariance matrix of the centered data. The covariance matrix summarizes the relationships between different features and quantifies how they vary together.\n",
    "\n",
    "   - The covariance matrix \\(\\Sigma\\) is an \\(n \\times n\\) matrix, where \\(n\\) is the number of features. The element in the \\(i\\)-th row and \\(j\\)-th column represents the covariance between the \\(i\\)-th and \\(j\\)-th features.\n",
    "\n",
    "2. **Eigenvalue Decomposition**:\n",
    "\n",
    "   - The next step is to perform eigenvalue decomposition on the covariance matrix. This involves finding the eigenvectors and eigenvalues.\n",
    "\n",
    "   - The eigenvectors represent the directions in the original feature space along which the data varies the most. The eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "3. **Selection of Principal Components**:\n",
    "\n",
    "   - The eigenvectors are ranked based on their corresponding eigenvalues in decreasing order. The eigenvectors with the highest eigenvalues are the principal components, as they capture the most significant sources of variation in the data.\n",
    "\n",
    "   - These principal components represent the directions of maximal variance in the original feature space.\n",
    "\n",
    "4. **Interpreting Principal Components**:\n",
    "\n",
    "   - The eigenvalues associated with each principal component indicate the amount of variance explained by that component. Larger eigenvalues imply that the corresponding principal component captures a greater portion of the total variance in the data.\n",
    "\n",
    "   - This information helps in understanding the relative importance of each principal component in explaining the variability in the data.\n",
    "\n",
    "5. **Reduced-Dimensional Representation**:\n",
    "\n",
    "   - The selected principal components form a basis for a lower-dimensional subspace onto which the data will be projected. This reduced-dimensional representation retains as much of the original variance as possible.\n",
    "\n",
    "   - The projection operation is performed by taking the dot product of the centered data with the selected eigenvectors.\n",
    "\n",
    "In summary, PCA uses the spread and variance of the data, as captured by the eigenvalues of the covariance matrix, to identify the principal components. The principal components represent the directions of maximal variance in the original feature space. By selecting a subset of these components, PCA enables dimensionality reduction while retaining the most important sources of variation in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ea1a0-80c7-4030-9bcd-3951ee86ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 9 \n",
    "# Ans --\n",
    "Principal Component Analysis (PCA) is well-suited for handling data with high variance in some dimensions and low variance in others. Here's how PCA addresses this situation:\n",
    "\n",
    "1. **Identifying Principal Components**:\n",
    "\n",
    "   - PCA identifies the directions (principal components) along which the data varies the most. These directions are determined by the eigenvectors of the covariance matrix.\n",
    "\n",
    "   - In cases where some dimensions have high variance while others have low variance, PCA will naturally prioritize capturing the variability along the dimensions with high variance.\n",
    "\n",
    "2. **Selection of Principal Components**:\n",
    "\n",
    "   - The eigenvectors corresponding to the dimensions with high variance will have larger eigenvalues, indicating that they capture a greater portion of the total variance in the data.\n",
    "\n",
    "   - As a result, these eigenvectors will be selected as principal components, while the eigenvectors associated with low-variance dimensions will have smaller eigenvalues and contribute less to the overall variability.\n",
    "\n",
    "3. **Dimensionality Reduction**:\n",
    "\n",
    "   - The selected principal components form a basis for a lower-dimensional subspace onto which the data will be projected. This subspace is defined by the directions that capture the most important sources of variation in the data.\n",
    "\n",
    "   - By projecting the data onto this subspace, PCA effectively reduces the dimensionality while retaining the most significant sources of variation. This can lead to a more compact and informative representation of the data.\n",
    "\n",
    "4. **Handling Irrelevant Dimensions**:\n",
    "\n",
    "   - In cases where some dimensions have very low variance (i.e., they essentially remain constant), PCA will recognize that these dimensions contribute little to the overall variability in the data.\n",
    "\n",
    "   - These low-variance dimensions will have small eigenvalues, indicating that they are not as important for capturing the underlying structure of the data.\n",
    "\n",
    "5. **Relevance to Data Analysis**:\n",
    "\n",
    "   - PCA is particularly useful in situations where some features are much more informative or influential than others. It allows the algorithm to focus on the dimensions that contain the most relevant information.\n",
    "\n",
    "   - This can be valuable in scenarios where certain features are noisy, redundant, or less informative for the analysis.\n",
    "\n",
    "In summary, PCA is adept at handling data with high variance in some dimensions and low variance in others. It automatically identifies and prioritizes the dimensions that capture the most important sources of variation, allowing for effective dimensionality reduction while retaining the key information in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
